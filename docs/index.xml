<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kcp – Docs</title><link>/docs/</link><description>Recent content in Docs on kcp</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Authorization</title><link>/docs/kcp-documenation/authorization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/authorization/</guid><description>
&lt;p>Within workspaces, KCP implements the same RBAC-based authorization mechanism as Kubernetes.
Other authorization schemes (i.e. ABAC) are not supported.
Generally, the same (cluster) role and (cluster) role binding principles apply exactly as in Kubernetes.&lt;/p>
&lt;p>In addition, additional RBAC semantics is implemented cross-workspaces, namely the following:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Top-Level Organization&lt;/strong> access: the user must have this as pre-requisite to access any other workspace, or is
even member and by that can create workspaces inside the organization workspace.&lt;/li>
&lt;li>&lt;strong>Workspace Content&lt;/strong> access: the user needs access to a workspace or is even admin.&lt;/li>
&lt;li>for some resources, additional permission checks are performed, not represented by local or Kubernetes standard RBAC rules. E.g.
&lt;ul>
&lt;li>workspace creation checks for organization membership (see above).&lt;/li>
&lt;li>workspace creation checks for &lt;code>use&lt;/code> verb on the &lt;code>ClusterWorkspaceType&lt;/code>.&lt;/li>
&lt;li>API binding via APIBinding objects requires verb &lt;code>bind&lt;/code> access to the corresponding &lt;code>APIExport&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>System Workspaces&lt;/strong> access: system workspaces are prefixed with &lt;code>system:&lt;/code> and are not accessible by users.&lt;/li>
&lt;/ul>
&lt;p>The details are outlined below.&lt;/p>
&lt;h1 id="authorizers">Authorizers&lt;/h1>
&lt;p>The following authorizers are configured in kcp:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Authorizer&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Top-Level organization authorizer&lt;/td>
&lt;td>checks that the user is allowed to access the organization&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Workspace content authorizer&lt;/td>
&lt;td>determines additional groups a user gets inside of a workspace&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>API binding authorizer&lt;/td>
&lt;td>validates the RBAC policy in the api exporters workspace&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Local Policy authorizer&lt;/td>
&lt;td>validates the RBAC policy in the workspace that is accessed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubernetes Bootstrap Policy authorizer&lt;/td>
&lt;td>validates the RBAC Kubernetes standard policy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>They are related in the following way:&lt;/p>
&lt;ol>
&lt;li>top-level organization authorizer must allow&lt;/li>
&lt;li>workspace content authorizer must allow, and adds additional (virtual per-request) groups to the request user influencing the follow authorizers.&lt;/li>
&lt;li>api binding authorizer must allow&lt;/li>
&lt;li>one of the local authorizer or bootstrap policy authorizer must allow.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span> ┌──────────────┐
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ┌─────────────────┐ ┌───────────────┐ ┌─────────────┐ ┌────►│ Local Policy ├──┐
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ Top-level │ │ │ │ │ │ │ authorizer │ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> request │ Organization │ │ Workspace ┌───┴───┐ │ API Binding │ │ │ │ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>──────────►│ authorizer ├────►│ Content │+groups├───►│ authorizer ├───┤ └──────────────┘ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ │ │ authorizer└───┬───┘ │ │ │ ▼
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ │ │ │ └─────────────┘ │ OR───►
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └─────────────────┘ └───────────────┘ │ ┌──────────────┐ ▲
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ │ Bootstrap │ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └────►│ Policy ├──┘
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ authorizer │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ │
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └──────────────┘
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;a href="https://asciiflow.com/#/share/eJyrVspLzE1VslLydg5QcCwtycgvyqxKLVLSUcpJrATSVkrVMUoVMUpWlpaGOjFKlUCWkaUZkFWSWlEC5MQoKdAAPJrS82hKA9FoQkxMHm2c0YQhgGoViQ5FuJhM3RPIsXgCFvXTdoE855OfnJijEJCfk5lcCVQyB0eAgpSG5Bfo5qSWpeaghQ1GGCGLoEtC%2BMhaE%2BFJDiYBDeOi1MLS1OISqKh%2FUXpiXmZVYklmfh667eH5RdnFBYnJqWie3IIebiDFjgGeCk6ZeSmZeelYXIPpWIhrCIQwJDBRvALWPweLKuf8vJLUPKi%2FtNOL8ksLilFUYhqGatASqOFTSEk3M7CmXfSYwxU1qJatQTWXUCxjgkfT9pDmEuwyJIbCDLxu8g9CjgF055EU1qiBQ4buGTjciBIqpBWQoEDfRPVSEiWOnPLzS4oVihILFFCyDrVtRA1MSGaBlWBQJfDcMoOW9QJqDqW%2BV5GsQhOgmVWkFSkxSrVKtQBJrg9s">ASCIIFlow document&lt;/a>&lt;/p>
&lt;h2 id="top-level-organization-authorizer">Top-Level Organization authorizer&lt;/h2>
&lt;p>An top-level organization is a workspace directly under root. When a user accesses a top-level organization or
a sub-workspace like &lt;code>root:org:ws:ws&lt;/code>, this authorizer will check in &lt;code>root&lt;/code> whether the user has permission
to the top-level org workspace represented by the &lt;code>ClusterWorkspace&lt;/code> named &lt;code>org&lt;/code> in &lt;code>root&lt;/code> with the following verbs:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Verb&lt;/th>
&lt;th>Resource&lt;/th>
&lt;th>Semantics&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>access&lt;/code>&lt;/td>
&lt;td>&lt;code>clusterworkspace/content&lt;/code>&lt;/td>
&lt;td>the user can access the organization &lt;code>root:org&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>E.g. the user is bound via a &lt;code>ClusterRoleBinding&lt;/code> in &lt;code>root&lt;/code> to a &lt;code>ClusterRole&lt;/code> of the following shape:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRole&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">root&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">rules&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">apiGroups&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">tenancy.kcp.dev&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">workspaces/content&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resourceNames&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">org&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">verbs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">access&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="workspace-content-authorizer">Workspace Content authorizer&lt;/h2>
&lt;p>The workspace content authorizer checks whether the user is granted &lt;code>admin&lt;/code> or &lt;code>access&lt;/code> verbs in
the parent workspace against the &lt;code>workspaces/content&lt;/code> resource with the &lt;code>resourceNames&lt;/code> of
the workspace being accessed.&lt;/p>
&lt;p>If any of the verbs is granted, the associated group is added to the user&amp;rsquo;s attributes
and will be evaluated in the subsequent authorizer chain.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Verb&lt;/th>
&lt;th>Groups&lt;/th>
&lt;th>Bootstrap cluster rolebinding&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>admin&lt;/code>&lt;/td>
&lt;td>&lt;code>system:kcp:workspace:admin&lt;/code> and &lt;code>system:kcp:workspace:access&lt;/code>&lt;/td>
&lt;td>&lt;code>system:kcp:clusterworkspace:admin&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>access&lt;/code>&lt;/td>
&lt;td>&lt;code>system:kcp:workspace:access&lt;/code>&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>kcp&amp;rsquo;s bootstrap policy provides default bindings:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRoleBinding&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">system:kcp:clusterworkspace:admin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">subjects&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Group&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">system:kcp:clusterworkspace:admin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">roleRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">apiGroup&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRole&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">cluster-admin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The access verb will not grant any cluster role but only associates with the &lt;code>system:kcp:clusterworkspace:access&lt;/code> group
and executes the subsequent authorizer chain.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;p>Given the user accesses &lt;code>root:org:ws:ws&lt;/code>, the verbs &lt;code>admin&lt;/code> and &lt;code>access&lt;/code> are asserted
against the &lt;code>workspaces/content&lt;/code> resource for the &lt;code>resourceNames: [&amp;quot;ws&amp;quot;]&lt;/code> in the workspace &lt;code>root:org:ws&lt;/code>.&lt;/p>
&lt;p>To give a user called &amp;ldquo;adam&amp;rdquo; admin access to a workspace &lt;code>root:org:ws:ws&lt;/code>, beyond having org access using the previous top-level organization authorizer,
a &lt;code>ClusterRole&lt;/code> must be created in &lt;code>root:org:ws&lt;/code> with the following shape:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRole&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workspace-admin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">root:org:ws&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">rules&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">apiGroups&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">tenancy.kcp.dev&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">workspaces/content&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">resourceNames&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">ws&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">verbs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">admin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and the user must be bound to it via a &lt;code>ClusterRoleBinding&lt;/code> in &lt;code>root:org:ws&lt;/code> like the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRoleBinding&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">adam-admin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">clusterName&lt;/span>: &lt;span style="color:#ae81ff">root:org:ws&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">subjects&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">User&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">adam&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">roleRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">apiGroup&lt;/span>: &lt;span style="color:#ae81ff">rbac.authorization.k8s.io&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ClusterRole&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">workspace-admin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="initializing-workspaces">Initializing Workspaces&lt;/h3>
&lt;p>By default, workspaces are only accessible to a user if they are in &lt;code>Ready&lt;/code> phase. Workspaces that are initializing
can be access only by users that are granted &lt;code>admin&lt;/code> verb on the &lt;code>workspaces/content&lt;/code> resource in the
parent workspace.&lt;/p>
&lt;p>Service accounts declared within a workspace don&amp;rsquo;t have access to initializing workspaces.&lt;/p>
&lt;h2 id="kubernetes-bootstrap-policy-authorizer">Kubernetes Bootstrap Policy authorizer&lt;/h2>
&lt;p>The bootstrap policy authorizer works just like the local authorizer but references RBAC rules
defined in the &lt;code>system:admin&lt;/code> system workspace.&lt;/p>
&lt;h2 id="local-policy-authorizer">Local Policy authorizer&lt;/h2>
&lt;p>Once the top-level organization authorizer and the workspace content authorizer granted access to a
workspace, RBAC rules contained in the workspace derived from the request context are evaluated.&lt;/p>
&lt;p>This authorizer ensures that RBAC rules contained within a workspace are being applied
and work just like in a regular Kubernetes cluster.&lt;/p>
&lt;p>Note: groups added by the workspace content authorizer can be used for role bindings in that workspace.&lt;/p>
&lt;p>It is possible to bind to roles and cluster roles in the bootstrap policy from a local policy &lt;code>RoleBinding&lt;/code> or &lt;code>ClusterRoleBinding&lt;/code>.&lt;/p>
&lt;h1 id="service-accounts">Service Accounts&lt;/h1>
&lt;p>Kubernetes service accounts are granted access to the workspaces they are defined in and that are ready.&lt;/p>
&lt;p>E.g. a service account &amp;ldquo;default&amp;rdquo; in &lt;code>root:org:ws:ws&lt;/code> is granted access to &lt;code>root:org:ws:ws&lt;/code>, and through the
workspace content authorizer it gains the &lt;code>system:kcp:clusterworkspace:access&lt;/code> group membership.&lt;/p></description></item><item><title>Docs: kcp Multi-Cluster Architecture</title><link>/docs/kcp-documenation/architecture/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/architecture/readme/</guid><description>
&lt;p>This doc describes the basic architecture of &lt;code>kcp&lt;/code> and other controllers in this repo that work with &lt;code>kcp&lt;/code> to implement the &lt;a href="../investigations/transparent-multi-cluster.md">transparent multi-cluster&lt;/a> demo.&lt;/p>
&lt;h2 id="kcp">&lt;code>kcp&lt;/code>&lt;/h2>
&lt;p>&lt;code>kcp&lt;/code> is a minimal Kubernetes API server, that only knows about basic types and &lt;code>CustomResourceDefinition&lt;/code>s (CRDs).
Its code is found in &lt;code>./cmd/kcp&lt;/code>.&lt;/p>
&lt;p>It&amp;rsquo;s responsible for storing data in a connected etcd cluster, and for enforcing RBAC on resources that it stores.
Its API allows clients to get, list, create, update, delete and watch resources.&lt;/p>
&lt;p>&lt;img alt="Diagram of kcp" width="30%" src="./kcp.png">&lt;/img>&lt;/p>
&lt;p>&lt;code>kcp&lt;/code> doesn&amp;rsquo;t know about most of the core Kubernetes types (Pods, etc.), and expects users or controllers to define them as needed, and to run controllers to respond to those resources.&lt;/p>
&lt;p>&lt;code>kcp&lt;/code> doesn&amp;rsquo;t support validating or mutating &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">admission controllers&lt;/a> at this time.&lt;/p>
&lt;p>&lt;code>kcp&lt;/code> is currently configured to create a new local etcd cluster at startup if one does not already exist.
In the future, flags will allow &lt;code>kcp&lt;/code> to connect to an existing remote etcd cluster.&lt;/p>
&lt;p>If all you want is a &lt;a href="../investigations/minimal-api-server.md">minimal API server&lt;/a>, that talks a Kubernetes-style API and stores and serves data for you, you can stop now.
The rest of this doc describes additional components you can run with &lt;code>kcp&lt;/code> to achieve transparent multi-cluster scheduling.&lt;/p>
&lt;h2 id="cluster-crd-and-cluster-controller">Cluster CRD and Cluster Controller&lt;/h2>
&lt;p>The example &lt;a href="../../config/workload.kcp.dev_clusters.yaml">Cluster&lt;/a> CRD type holds information to reach and authenticate to another Kubernetes cluster&amp;rsquo;s API server.
To do this, its &lt;code>.spec.kubeconfig&lt;/code> field defines a YAML serialized &lt;a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig&lt;/a> file.&lt;/p>
&lt;p>&lt;strong>NB:&lt;/strong> A kubeconfig contains sensitive information to authenticate with a cluster&amp;rsquo;s API server.
In the fullness of time we need a more secure mechanism for registering and authenticating to a cluster.&lt;/p>
&lt;p>With the Cluster type defined, users can create Cluster resources to tell &lt;code>kcp&lt;/code> about their clusters.&lt;/p>
&lt;p>The Cluster Controller (&lt;code>./cmd/cluster-controller&lt;/code>) connects to &lt;code>kcp&lt;/code> and watches for new Cluster resources that get defined.
When a new resource is seen, the controller uses its &lt;code>.spec.kubeconfig&lt;/code> to connect to the cluster and start a &lt;a href="#syncer">Syncer&lt;/a>.&lt;/p>
&lt;h3 id="crd-puller">CRD Puller&lt;/h3>
&lt;p>Before starting the Syncer, and continuously while it&amp;rsquo;s connected to the cluster, the Cluster Controller watches the cluster&amp;rsquo;s API resources to discover new types and updates to existing types.
The Cluster Controller uses this information to negotiate possible incompatible CRD type definitions, to determine whether an incoming resource can be sent to a cluster&amp;rsquo;s Syncer.&lt;/p>
&lt;p>&lt;img alt="Diagram of kcp and Cluster Controller" width="50%" src="./cluster-controller.png">&lt;/img>&lt;/p>
&lt;p>&lt;strong>NB:&lt;/strong> In these diagrams, controllers are depicted as separate, external boxes.
In reality, these control loops could be compiled in and embedded into the same binary as &lt;code>kcp&lt;/code>, or run as remote external services.&lt;/p>
&lt;h2 id="syncer">Syncer&lt;/h2>
&lt;p>The Syncer (&lt;code>./cmd/syncer&lt;/code>) maintains a connection to the &lt;code>kcp&lt;/code>, and to a Kubernetes cluster&amp;rsquo;s API server.&lt;/p>
&lt;p>After initial type negotiation, the Syncer watches for resources of all types that are scheduled to that cluster, using the &lt;code>workloads.kcp.dev/cluster&lt;/code> label, and copies those resources to the Kubernetes cluster.&lt;/p>
&lt;p>It also watches for updates to resources in its cluster, and mirrors any updates to &lt;code>.status&lt;/code> to the &lt;code>kcp&lt;/code>&amp;rsquo;s API.&lt;/p>
&lt;p>&lt;img alt="Diagram of kcp, Cluster Controller and Syncer" src="./syncer.png">&lt;/img>&lt;/p>
&lt;p>&lt;strong>NB:&lt;/strong> Syncer can run in one of three modes, determined by a flag given to the Cluster Controller that starts Syncers:&lt;/p>
&lt;ol>
&lt;li>In &lt;code>--pull&lt;/code> mode (pictured), the Syncer runs as a Pod inside the downstream cluster, communicating with the cluster&amp;rsquo;s API server from inside the cluster.&lt;/li>
&lt;li>In &lt;code>--push&lt;/code> mode, Syncers run as goroutines in the Cluster Controller binary, communicating with the cluster&amp;rsquo;s API server from outside the cluster.&lt;/li>
&lt;li>In &amp;ldquo;none&amp;rdquo; mode, operators are expected to run the Syncer as a separate process. This is mainly to aid debugging.&lt;/li>
&lt;/ol>
&lt;p>There is ongoing discussion about where Syncers should run, and the answer might be &amp;ldquo;either inside or outside the cluster&amp;rdquo;.
Its code should be structured to be agnostic to this distinction, since it only needs to be given two kubeconfigs, wherever it runs.&lt;/p>
&lt;p>If the in-cluster Syncer ever becomes unreachable, or out-of-cluster Syncer fails to reach the downstream cluster, the Cluster&amp;rsquo;s &lt;code>.status.conditions&lt;/code> is update to indicate that the Cluster is not &lt;code>Ready&lt;/code>.&lt;/p>
&lt;h2 id="deployment-splitter">Deployment Splitter&lt;/h2>
&lt;p>The Deployment Splitter (&lt;code>./cmd/deployment-splitter&lt;/code>) is an example of a very simple multi-cluster resource scheduler.
It watches for &lt;code>Deployment&lt;/code> resources in the &lt;code>kcp&lt;/code>, and determines how many of that Deployment&amp;rsquo;s &lt;code>replicas&lt;/code> should be scheduled to which of the available Cluster resources.&lt;/p>
&lt;p>It currently does this very &lt;em>very&lt;/em> simply, by dividing the number of &lt;code>replicas&lt;/code> evenly across available clusters.&lt;/p>
&lt;p>When the Deployment Splitter splits a Deployment, it creates N new Deployment resources, each labeled for an available cluster (i.e., it labels each with &lt;code>workloads.kcp.dev/cluster: my-cluster-name&lt;/code>).
This in turn instructs the &lt;a href="#syncer">Syncer&lt;/a> for that cluster to see the Deployment shard and sync it down to the cluster.&lt;/p>
&lt;p>&lt;img alt="Diagram of kcp, Cluster Controller, Syncer and Deployment Splitter" src="./deployment-splitter.png">&lt;/img>&lt;/p>
&lt;p>The Deployment Splitter is &lt;em>very simple&lt;/em>, and would need lots of improvements to make it usable in a real production scenario:&lt;/p>
&lt;ul>
&lt;li>it should take into account whether the Cluster reports as &lt;code>Ready&lt;/code>, in case the Syncer is having difficulty syncing to the cluster.&lt;/li>
&lt;li>it should take into account user provided scheduling hints such as affinity, anti-affinity, etc.&lt;/li>
&lt;li>it should take into account resources&amp;rsquo; dependencies and schedule dependent resources to the same cluster.&lt;/li>
&lt;li>it should continuously watch for new Cluster resources, to take advantage of those new resources and rebalance existing scheduling decisions.&lt;/li>
&lt;li>it should continuously watch for existing Cluster resources to report as not &lt;code>Ready&lt;/code>, to unschedule resources from those clusters.&lt;/li>
&lt;li>it should become more generic, so that it can schedule resources of all types (e.g., &lt;code>DaemonSet&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>PersistentVolume&lt;/code>s, CRDs of all kinds).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>Taken together, these components are designed to work in concert to provide a robust system for scheduling generic resources across multiple clusters.&lt;/p></description></item><item><title>Docs: Cluster Mapper</title><link>/docs/kcp-documenation/cluster-mapper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/cluster-mapper/</guid><description>
&lt;p>mapper =&lt;/p>
&lt;p>Invariants:&lt;/p>
&lt;ol>
&lt;li>Every object assigned by a location exists on the target location&lt;/li>
&lt;li>Every object in the target location has appropriate metadata set indicating source&lt;/li>
&lt;li>Every object in the target location that has status and the appropriate policy choice set will reflect that status back in the source object&lt;/li>
&lt;li>No object exists in the target location that is not assigned by a source object(s)&lt;/li>
&lt;/ol>
&lt;p>1..N mappers per location (sharding by virtual cluster?)
mappers see only the objects assigned to them (special API)&lt;/p>
&lt;ul>
&lt;li>can load policy info during mapping?&lt;/li>
&lt;li>can load policy info from side channels like regular resources
wants to watch many different resources at once and deal with them as unstructured&lt;/li>
&lt;/ul>
&lt;p>assumption: all resources in the location are compatible with the target API (managed by control plane and CRD folding), and if that is broken the mapper is instructed to halt mapping&lt;/p>
&lt;ul>
&lt;li>how to deal with partial mapping when one object is broken&lt;/li>
&lt;li>how does CRD folding actually work (separate doc)&lt;/li>
&lt;/ul>
&lt;p>assumption: higher level control (admission) manages location accessibility&lt;/p>
&lt;p>assumption: kcp has 1k virtual clusters with 50k resources, a given mapper may see 1k to 50k resources&lt;/p>
&lt;ul>
&lt;li>fully syncing will take &lt;code>50k / default throttle&lt;/code> (50-100 req/s) ~ 1000s in serial&lt;/li>
&lt;li>order may be important&lt;/li>
&lt;li>there may be 100-1k locations, so we may have up to 1/1000 cardinality (implies indexing)&lt;/li>
&lt;li>mappers that favor summarization objects (deployments) have scale advantages over those that don&amp;rsquo;t (pods)&lt;/li>
&lt;/ul>
&lt;p>Assumption: we prefer not to require order to correctly map, but some order is implicit due to resource version ordering&lt;/p>
&lt;p>basic sync loop:&lt;/p>
&lt;ol>
&lt;li>retrieve all objects assigned to a particular cluster (metadata.annotations[&amp;ldquo;kcp.io/assigned-locations&amp;rdquo;] = [&amp;ldquo;a&amp;rdquo;,&amp;ldquo;b&amp;rdquo;])&lt;/li>
&lt;li>transform them into one or more objects for the destination
&lt;ul>
&lt;li>add labels?&lt;/li>
&lt;li>map namespace from source to target&lt;/li>
&lt;li>hide certain annotations (assigned-locations?)&lt;/li>
&lt;li>set and maintain other annotations (like the source namespace / virtual cluster)&lt;/li>
&lt;li>set a controller ref?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>perform a merge into the destination object (overwrite of spec)&lt;/li>
&lt;li>sync some fields (status?) back to source object&lt;/li>
&lt;li>delete all objects no longer assigned to the remote location
&lt;ul>
&lt;li>read all mappable objects from all mappable resources?
&lt;ul>
&lt;li>use &lt;code>kcp.dev/location=X&lt;/code> label to filter&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>detect when an object policy on mapping is changed?&lt;/li>
&lt;li>only need the partial object (metadata)&lt;/li>
&lt;li>can we leverage the garbage collector to delete the object?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Kubectl KCP Plugin</title><link>/docs/kcp-documenation/kubectl-kcp-plugin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/kubectl-kcp-plugin/</guid><description>
&lt;p>kcp provides a kubectl plugin that simplifies the operations with the kcp server.&lt;/p>
&lt;p>You can install the plugin from the current repo:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ make install
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go install ./cmd/...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The plugin will be &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">automatically discovered by your current &lt;code>kubectl&lt;/code> binary&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kubectl-kcp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>KCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters &lt;span style="color:#66d9ef">for&lt;/span> resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration &lt;span style="color:#66d9ef">for&lt;/span> individual teams without having access to the underlying clusters.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>This command provides KCP specific sub-command &lt;span style="color:#66d9ef">for&lt;/span> kubectl.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Usage:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kcp &lt;span style="color:#f92672">[&lt;/span>command&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Available Commands:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> completion generate the autocompletion script &lt;span style="color:#66d9ef">for&lt;/span> the specified shell
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> help Help about any command
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workspace Manages KCP workspaces
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Flags:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --add_dir_header If true, adds the file directory to the header of the log messages
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --alsologtostderr log to standard error as well as files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -h, --help help &lt;span style="color:#66d9ef">for&lt;/span> kcp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace &lt;span style="color:#f92672">(&lt;/span>default :0&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --log_dir string If non-empty, write log files in this directory
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --log_file string If non-empty, use this log file
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --log_file_max_size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. &lt;span style="color:#f92672">(&lt;/span>default 1800&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --logtostderr log to standard error instead of files &lt;span style="color:#f92672">(&lt;/span>default true&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --one_output If true, only write logs to their native severity level &lt;span style="color:#f92672">(&lt;/span>vs also writing to each lower severity level&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --skip_headers If true, avoid header prefixes in the log messages
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --skip_log_headers If true, avoid headers when opening log files
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --stderrthreshold severity logs at or above this threshold go to stderr &lt;span style="color:#f92672">(&lt;/span>default 2&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -v, --v Level number &lt;span style="color:#66d9ef">for&lt;/span> the log level verbosity
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --vmodule moduleSpec comma-separated list of pattern&lt;span style="color:#f92672">=&lt;/span>N settings &lt;span style="color:#66d9ef">for&lt;/span> file-filtered logging
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Use &lt;span style="color:#e6db74">&amp;#34;kcp [command] --help&amp;#34;&lt;/span> &lt;span style="color:#66d9ef">for&lt;/span> more information about a command.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Locations and Scheduling</title><link>/docs/kcp-documenation/locations-and-scheduling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/locations-and-scheduling/</guid><description>
&lt;p>KCP implements &lt;em>Compute as a Service&lt;/em> via a concept of Transparent Multi Cluster (TMC). TMC means that
Kubernetes clusters are attached to a kcp installation to execute workload objects from the users'
workspaces by syncing these workload objects down to those clusters and the objects&amp;rsquo; status
back up. This gives the illusion of native compute in KCP.&lt;/p>
&lt;p>We call it &lt;em>Compute as a Service&lt;/em> because the registered &lt;code>SyncTargets&lt;/code> live in workspaces that
are (normally) invisible to the users, and the teams operating compute can be different from
the compute consumers.&lt;/p>
&lt;p>The APIs used for Compute as a Service are:&lt;/p>
&lt;ol>
&lt;li>&lt;code>scheduling.kcp.dev/v1alpha1&lt;/code> – we call the outcome of this &lt;em>placement&lt;/em> of namespaces.&lt;/li>
&lt;li>&lt;code>workload.kcp.dev/v1alpha1&lt;/code> – responsible for the syncer component of TMC.&lt;/li>
&lt;/ol>
&lt;h2 id="main-concepts">Main Concepts&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;code>SyncTarget&lt;/code> in &lt;code>workload.kcp.dev/v1alpha1&lt;/code> – representations of Kubernetes clusters that are attached to a kcp installation to
execute workload objects from the users&amp;rsquo; workspaces. On a Kubernetes cluster, there is one syncer
process for each &lt;code>SyncTarget&lt;/code> object.&lt;/p>
&lt;p>Sync targets are invisible to users, and (medium term) at most identified via a UID.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Location&lt;/code> in &lt;code>scheduling.kcp.dev/v1alpha1&lt;/code> – represents a collection of &lt;code>SyncTarget&lt;/code> objects selected via instance labels, and
exposes labels (potentially different from the instance labels) to the users to describe, identify and select locations to be used
for placement of user namespaces onto sync targets.&lt;/p>
&lt;p>Locations are visible to users, but owned by the compute service team, i.e. read-only to the users and only projected
into their workspaces for visibility. A placement decision references a location by name.&lt;/p>
&lt;p>&lt;code>SyncTarget&lt;/code>s in a &lt;code>Location&lt;/code> are transparent to the user. Workloads should be able to seamlessly move from one &lt;code>SyncTarget&lt;/code> to another
within a &lt;code>Location&lt;/code>, based on operational concerns of the compute service provider, like decommissioning a cluster, rebalancing
capacity, or due to an outage of a cluster.&lt;/p>
&lt;p>It is compute service&amp;rsquo;s responsibility to ensure that for workloads in a location, to the user it looks like ONE cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Placement&lt;/code> in &lt;code>scheduling.kcp.dev/v1alpha1&lt;/code> – represents a selection rule to choose ONE &lt;code>Location&lt;/code> via location labels, and bind
the selected location to MULTIPLE namespaces in a user workspace. For Workspaces with multiple Namespaces, users can create multiple
Placements to assign specific Namespace(s) to specific Locations.&lt;/p>
&lt;p>&lt;code>Placement&lt;/code> are visible and writable to users. A default &lt;code>Placement&lt;/code> is automatically created when a workload &lt;code>APIBinding&lt;/code> is
created on the user workspace, which randomly select a &lt;code>Location&lt;/code> and bind to all namespaces in this workspace. The user can mutate
or delete the default &lt;code>Placement&lt;/code>. The corresponding &lt;code>APIBinding&lt;/code> will be annotated with &lt;code>workload.kcp.dev/skip-default-object-creation&lt;/code>,
so that the default &lt;code>Placement&lt;/code> will not be recreated upon deletion.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Compute Service Workspace&lt;/em> (previously &lt;em>Negotiation Workspace&lt;/em>) – the workspace owned by the compute service team to hold
the &lt;code>APIExport&lt;/code> (named &lt;code>kubernetes&lt;/code> today) with the synced resources, and &lt;code>SyncTarget&lt;/code> and &lt;code>Location&lt;/code> objects.&lt;/p>
&lt;p>The user binds to the &lt;code>APIExport&lt;/code> called &lt;code>kubernetes&lt;/code> using an &lt;code>APIBinding&lt;/code>. From this moment on, the users&amp;rsquo; workspaces
are subject to placement.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note: binding to a compute service is a permanent decision. Unbinding (i.e. deleting of the APIBinding object) means deletion of the
workload objects.&lt;/p>
&lt;p>Note: it is planned to allow multiple location workspaces for the same compute service, even with different owners.&lt;/p>
&lt;h3 id="placement-and-resource-scheduling">Placement and resource scheduling&lt;/h3>
&lt;p>The placement state is one of&lt;/p>
&lt;ul>
&lt;li>&lt;code>Pending&lt;/code> – the placement controller waits for a valid &lt;code>Location&lt;/code> to select&lt;/li>
&lt;li>&lt;code>Bound&lt;/code> – at least one namespace is bound to the placement. When the user updates the spec of the &lt;code>Placement&lt;/code>, the selected location of
the placement will be changed in &lt;code>Bound&lt;/code> state.&lt;/li>
&lt;li>&lt;code>Unbound&lt;/code> – a location is selected by the placement, but no namespace is bound to the placement. When the user updates the spec of the &lt;code>Placement&lt;/code>, the
selected location of the placement will be changed in &lt;code>Unbound&lt;/code> state.&lt;/li>
&lt;/ul>
&lt;p>Note: sync targets from different locations can be bound at the same time, while each location can only have one sync target bound to the
namespace.&lt;/p>
&lt;p>The user interface to influence the placement decisions is the &lt;code>Placement&lt;/code> object. For example, use can create a placement to bind namespace with
label of &amp;ldquo;app=foo&amp;rdquo; to a location with label &amp;ldquo;cloud=aws&amp;rdquo; as below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">scheduling.kcp.dev/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Placement&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">aws&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">locationSelectors&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cloud&lt;/span>: &lt;span style="color:#ae81ff">aws&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespaceSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">foo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">locationWorkspace&lt;/span>: &lt;span style="color:#ae81ff">root:default:location-ws&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A matched location will be selected for this &lt;code>Placement&lt;/code> at first, which makes the &lt;code>Placement&lt;/code> turns from &lt;code>Pending&lt;/code> to &lt;code>Unbound&lt;/code>. Then if there is at
least one matching Namespace, the Namespace will be annotated with &lt;code>scheduling.kcp.dev/placement&lt;/code> and the placement turns from &lt;code>Unbound&lt;/code> to &lt;code>Bound&lt;/code>.
After this, a &lt;code>SyncTarget&lt;/code> will be selected from the location picked by the placement. &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label with value of &lt;code>Sync&lt;/code> will be set if a valid &lt;code>SyncTarget&lt;/code> is selected.&lt;/p>
&lt;p>The user can create another placement targeted to a different location for this Namespace, e.g.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">scheduling.kcp.dev/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Placement&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">gce&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">locationSelectors&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">cloud&lt;/span>: &lt;span style="color:#ae81ff">gce&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">namespaceSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">foo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">locationWorkspace&lt;/span>: &lt;span style="color:#ae81ff">root:default:location-ws&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which will result in another &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label added to the Namespace, and the Namespace will have two different
&lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label.&lt;/p>
&lt;p>Placement is in the &lt;code>Ready&lt;/code> status condition when&lt;/p>
&lt;ol>
&lt;li>selected location matches the &lt;code>Placement&lt;/code> spec.&lt;/li>
&lt;li>selected location exists in the location workspace.&lt;/li>
&lt;/ol>
&lt;h4 id="sync-target-removing">Sync target removing&lt;/h4>
&lt;p>A sync target will be removed when:&lt;/p>
&lt;ol>
&lt;li>corresponding &lt;code>Placement&lt;/code> is deleted.&lt;/li>
&lt;li>corresponding &lt;code>Placement&lt;/code> is not in &lt;code>Ready&lt;/code> condition.&lt;/li>
&lt;li>corresponding &lt;code>SyncTarget&lt;/code> is evicting/not Ready/deleted&lt;/li>
&lt;/ol>
&lt;p>All above cases will make the &lt;code>SyncTraget&lt;/code> represented in the label &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> invalid, which will cause
&lt;code>finalizers.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> annotation with removing time in the format of RFC-3339 added on the Namespace.&lt;/p>
&lt;h3 id="resource-syncing">Resource Syncing&lt;/h3>
&lt;p>As soon as the &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label is set on the Namespace, the workload resource controller will
copy the &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label to the resources in that namespace.&lt;/p>
&lt;p>Note: in the future, the label on the resources is first set to empty string &lt;code>&amp;quot;&amp;quot;&lt;/code>, and a coordination controller will be
able to apply changes before syncing starts. This includes the ability to add per-location finalizers through the
&lt;code>finalizers.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> annotation such that the coordination controller gets full control over
the downstream life-cycle of the objects per location (imagine an ingress that blocks downstream removal until the new replicas
have been launched on another sync target). Finally, the coordination controller will replace the empty string with &lt;code>Sync&lt;/code>
such that the state machine continues.&lt;/p>
&lt;p>With the state label set to &lt;code>Sync&lt;/code>, the syncer will start seeing the resources in the namespace
and starts syncing them downstream, first by creating the namespace. Before syncing, it will also set
a finalizer &lt;code>workload.kcp.dev/syncer-&amp;lt;cluster-id&amp;gt;&lt;/code> on the upstream object in order to delay upstream deletion until
the downstream object is also deleted.&lt;/p>
&lt;p>When the &lt;code>deletion.internal.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> is added to the Namespace. The virtual workspace apiserver
will translate that annotation into a deletion timestamp on the object the syncer sees. The syncer
notices that as a started deletion flow. As soon as there are no coordination controller finalizers registered via the
&lt;code>finalizers.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> annotation anymore, the syncer will start a deletion of the downstream object.&lt;/p>
&lt;p>When the downstream deletion is complete, the syncer will remove the finalizer from the upstream object, and the
&lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> labels gets deleted as well. The syncer stops seeing the object in the virtual
workspace.&lt;/p>
&lt;p>Note: there is a missing bit in the implementation (in v0.5) about removal of the &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code>
label from namespaces: the syncer currently does not participate in the namespace deletion state-machine, but has to and signal finished
downstream namespace deletion via &lt;code>state.workload.kcp.dev/&amp;lt;cluster-id&amp;gt;&lt;/code> label removal.&lt;/p></description></item><item><title>Docs: Registering kubernetes clusters using syncers</title><link>/docs/kcp-documenation/syncer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/syncer/</guid><description>
&lt;p>In order to register a Kubernetes clusters with the kcp server,
users have to install a special component named &lt;a href="https://github.com/kcp-dev/kcp/tree/main/docs/architecture#syncer">syncer&lt;/a>.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>kcp server&lt;/li>
&lt;li>&lt;a href="./kubectl-kcp-plugin.md">kcp kubectl plugin&lt;/a>&lt;/li>
&lt;li>kubernetes cluster&lt;/li>
&lt;/ul>
&lt;h2 id="instructions">Instructions&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>(Optional) Skip this step, if you already have a physical cluster.
Create a kind cluster to back the sync target:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kind create cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Creating cluster &lt;span style="color:#e6db74">&amp;#34;kind&amp;#34;&lt;/span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;snip&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Set kubectl context to &lt;span style="color:#e6db74">&amp;#34;kind-kind&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>You can now use your cluster with:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl cluster-info --context kind-kind
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong> that this step sets current context to the new kind cluster.
Make sure to use a KCP kubeconfig for the next steps unless told otherwise.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create an organisation and immediately enter it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kubectl kcp workspace create my-org --enter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> created. Waiting &lt;span style="color:#66d9ef">for&lt;/span> it to be ready...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> is ready to use.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Current workspace is &lt;span style="color:#e6db74">&amp;#34;root:my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type &lt;span style="color:#e6db74">&amp;#34;root:organization&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Enable the syncer for a p-cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl kcp workload sync &amp;lt;mycluster&amp;gt; --syncer-image &amp;lt;image name&amp;gt; -o syncer.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Where &lt;code>&amp;lt;image name&amp;gt;&lt;/code> &lt;a href="https://github.com/kcp-dev/kcp/pkgs/container/kcp%2Fsyncer">one of the syncer images&lt;/a> for your corresponding KCP release (e.g. &lt;code>ghcr.io/kcp-dev/kcp/syncer:v0.7.5&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Apply the manifest to the p-cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ KUBECONFIG&lt;span style="color:#f92672">=&lt;/span>&amp;lt;pcluster-config&amp;gt; kubectl apply -f syncer.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespace/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>serviceaccount/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret/kcp-syncer-kind-1owee1ci-token created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment.apps/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and it will create a &lt;code>kcp-syncer&lt;/code> deployment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ KUBECONFIG&lt;span style="color:#f92672">=&lt;/span>&amp;lt;pcluster-config&amp;gt; kubectl -n kcp-syncer-kind-1owee1ci get deployments
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kcp-syncer 1/1 &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> 13m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Wait for the kcp sync target to go ready:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for&lt;span style="color:#f92672">=&lt;/span>condition&lt;span style="color:#f92672">=&lt;/span>Ready synctarget/&amp;lt;mycluster&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="running-a-workload">Running a workload&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Create a deployment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl create deployment kuard --image gcr.io/kuar-demo/kuard-amd64:blue
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note:&lt;/strong> replace &amp;ldquo;gcr.io/kuar-demo/kuard-amd64:blue&amp;rdquo; with
&amp;ldquo;gcr.io/kuar-demo/kuard-arm64:blue&amp;rdquo; in case you&amp;rsquo;re running
an Apple M1 based virtual machine.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify the deployment on the local workspace:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kubectl rollout status deployment/kuard
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Waiting &lt;span style="color:#66d9ef">for&lt;/span> deployment &lt;span style="color:#e6db74">&amp;#34;kuard&amp;#34;&lt;/span> rollout to finish: &lt;span style="color:#ae81ff">0&lt;/span> of &lt;span style="color:#ae81ff">1&lt;/span> updated replicas are available...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment &lt;span style="color:#e6db74">&amp;#34;kuard&amp;#34;&lt;/span> successfully rolled out
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="for-syncer-development">For syncer development&lt;/h2>
&lt;h3 id="running-in-a-kind-cluster-with-a-local-registry">Running in a kind cluster with a local registry&lt;/h3>
&lt;p>You can run the syncer in a kind cluster for development.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a &lt;code>kind&lt;/code> cluster with a local registry to simplify syncer development
by executing the following script:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>/bin/bash -c &lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>curl -fsSL https://raw.githubusercontent.com/kubernetes-sigs/kind/main/site/static/examples/kind-with-registry.sh&lt;span style="color:#66d9ef">)&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Install &lt;code>ko&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>go install github.com/google/ko@latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Build image and push to the local registry integrated with &lt;code>kind&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>KO_DOCKER_REPO&lt;span style="color:#f92672">=&lt;/span>localhost:5001 ko publish ./cmd/syncer -t &amp;lt;image tag&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By default &lt;code>ko&lt;/code> will build for &lt;code>amd64&lt;/code>. To build for &lt;code>arm64&lt;/code>
(e.g. apple silicon), specify &lt;code>--platform=linux/arm64&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create an organisation and immediately enter it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kubectl kcp workspace create my-org --enter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> created. Waiting &lt;span style="color:#66d9ef">for&lt;/span> it to be ready...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> is ready to use.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Current workspace is &lt;span style="color:#e6db74">&amp;#34;root:my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type &lt;span style="color:#e6db74">&amp;#34;root:organization&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>To use the image pushed to the local registry, supply &lt;code>&amp;lt;image name&amp;gt;&lt;/code> to the
&lt;code>kcp workload sync&lt;/code> plugin command, where &lt;code>&amp;lt;image name&amp;gt;&lt;/code> is
from the output of &lt;code>ko publish&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl kcp workload sync &amp;lt;mycluster&amp;gt; --syncer-image &amp;lt;image name&amp;gt; -o syncer.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Apply the manifest to the p-cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ KUBECONFIG&lt;span style="color:#f92672">=&lt;/span>&amp;lt;pcluster-config&amp;gt; kubectl apply -f syncer.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespace/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>serviceaccount/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret/kcp-syncer-kind-1owee1ci-token created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment.apps/kcp-syncer-kind-1owee1ci created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and it will create a &lt;code>kcp-syncer&lt;/code> deployment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ KUBECONFIG&lt;span style="color:#f92672">=&lt;/span>&amp;lt;pcluster-config&amp;gt; kubectl -n kcp-syncer-kind-1owee1ci get deployments
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kcp-syncer 1/1 &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span> 13m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Wait for the kcp sync target to go ready:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for&lt;span style="color:#f92672">=&lt;/span>condition&lt;span style="color:#f92672">=&lt;/span>Ready synctarget/&amp;lt;mycluster&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="running-locally">Running locally&lt;/h3>
&lt;p>TODO(m1kola): we need a less hacky way to run locally: needs to be more close
to what we have when running inside the kind with own kubeconfig.&lt;/p>
&lt;p>This assumes that KCP is also being run locally.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a kind cluster to back the sync target:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kind create cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Creating cluster &lt;span style="color:#e6db74">&amp;#34;kind&amp;#34;&lt;/span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;snip&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Set kubectl context to &lt;span style="color:#e6db74">&amp;#34;kind-kind&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>You can now use your cluster with:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl cluster-info --context kind-kind
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Make sure to use kubeconfig for your local KCP:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export KUBECONFIG&lt;span style="color:#f92672">=&lt;/span>.kcp/admin.kubeconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Create an organisation and immediately enter it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kubectl kcp workspace create my-org --enter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> created. Waiting &lt;span style="color:#66d9ef">for&lt;/span> it to be ready...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Workspace &lt;span style="color:#e6db74">&amp;#34;my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type root:organization&lt;span style="color:#f92672">)&lt;/span> is ready to use.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Current workspace is &lt;span style="color:#e6db74">&amp;#34;root:my-org&amp;#34;&lt;/span> &lt;span style="color:#f92672">(&lt;/span>type &lt;span style="color:#e6db74">&amp;#34;root:organization&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Enable the syncer for a p-cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl kcp workload sync &amp;lt;mycluster&amp;gt; --syncer-image &amp;lt;image name&amp;gt; -o syncer.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>&amp;lt;image name&amp;gt;&lt;/code> can be anything here as it will only be used to generate &lt;code>syncer.yaml&lt;/code> which we are not going to apply.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Gather data required for the syncer:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>syncTargetName&lt;span style="color:#f92672">=&lt;/span>&amp;lt;mycluster&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>syncTargetUID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>kubectl get synctarget $syncTargetName -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;{.metadata.uid}&amp;#34;&lt;/span>&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>fromCluster&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>kubectl ws current --short&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Run the following snippet:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>go run ./cmd/syncer &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-kubeconfig&lt;span style="color:#f92672">=&lt;/span>.kcp/admin.kubeconfig &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-context&lt;span style="color:#f92672">=&lt;/span>base &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --to-kubeconfig&lt;span style="color:#f92672">=&lt;/span>$HOME/.kube/config &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --sync-target-name&lt;span style="color:#f92672">=&lt;/span>$syncTargetName &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --sync-target-uid&lt;span style="color:#f92672">=&lt;/span>$syncTargetUID &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --from-cluster&lt;span style="color:#f92672">=&lt;/span>$fromCluster &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --resources&lt;span style="color:#f92672">=&lt;/span>configmaps &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --resources&lt;span style="color:#f92672">=&lt;/span>deployments.apps &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --resources&lt;span style="color:#f92672">=&lt;/span>secrets &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --resources&lt;span style="color:#f92672">=&lt;/span>serviceaccounts &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --qps&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">30&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --burst&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">20&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Wait for the kcp sync target to go ready:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for&lt;span style="color:#f92672">=&lt;/span>condition&lt;span style="color:#f92672">=&lt;/span>Ready synctarget/&amp;lt;mycluster&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol></description></item><item><title>Docs: Terminology for control plane</title><link>/docs/kcp-documenation/terminology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/terminology/</guid><description>
&lt;p>Contains the definitions shared across design documents around prototyping a kube-like control plane (in KCP). This is
a derivative work of other design documents intended to frame terminology. All future statements that may be changed by
designs is covered by those designs, and not duplicated here.&lt;/p>
&lt;h2 id="logical-cluster">Logical cluster&lt;/h2>
&lt;p>A logical cluster is a way to subdivide a single kube-apiserver + etcd storage into multiple clusters (different APIs,
separate semantics for access, policy, and control) without requiring multiple instances. A logical cluster is a
mechanism for achieving separation, but may be modelled differently in different use cases. A logical cluster is
similar to a virtual cluster as defined by sig-multicluster, but is able to amortize the cost of a new cluster to be
zero or near-zero memory and storage so that we can create tens of millions of empty clusters cheaply.&lt;/p>
&lt;p>A logical cluster is a storage level concept that adds an additional attribute to an object’s identifier on a
kube-apiserver. Regular servers identify objects by (group, version, resource, optional namespace, name). A logical
cluster enriches an identifier: (group, version, resource, &lt;strong>logical cluster name&lt;/strong>, optional namespace, name).&lt;/p>
&lt;h2 id="workload-cluster">Workload Cluster&lt;/h2>
&lt;p>A physical cluster is a “real Kubernetes cluster”, i.e. one that can run Kubernetes workloads and accepts standard
Kubernetes API objects. For the near term, it is assumed that a physical cluster is a distribution of Kubernetes and
passes the conformance tests and exposes the behavior a regular Kubernetes admin or user expects.&lt;/p>
&lt;h2 id="workspace">Workspace&lt;/h2>
&lt;p>A workspace models a set of user-facing APIs for CRUD. Each workspace is backed by a logical cluster, but not all
logical clusters may be exposed as workspaces. Creating a Workspace object results in a logical cluster being available
via a URL for the client to connect and create resources supported by the APIs in that workspace. There could be
multiple different models that result in logical clusters being created, with different policies or lifecycles, but
Workspace is intended to be the most generic representation of the concept with the broadest possible utility to anyone
building control planes.&lt;/p>
&lt;p>A workspace binds APIs and makes them accessible inside the logical cluster, allocates capacity for creating instances
of those APIs (quota), and defines how multi-workspace operations can be performed by users, clients, and controller
integrations.&lt;/p>
&lt;p>To a user, a workspace appears to be a Kubernetes cluster minus all the container orchestration specific resources. It
has its own discovery, its own OpenAPI spec, and follows the kube-like constraints about uniqueness of
Group-Version-Resource and its behaviour (no two GVRs with different schemas can exist per workspace, but workspaces can
have different schemas). A user can define a workspace as a context in a kubeconfig file and &lt;code>kubectl get all -A&lt;/code> would
return all objects in all namespaces of that workspace.&lt;/p>
&lt;p>Workspace naming is chosen to be aligned with the Kubernetes Namespace object - a Namespace subdivides a workspace by
name, a workspace subdivides the universe into chunks of meaningful work.&lt;/p>
&lt;p>Workspaces are the containers for all API objects, so users orient by viewing lists of workspaces from APIs.&lt;/p>
&lt;h2 id="workspace-type">Workspace type&lt;/h2>
&lt;p>Workspaces have types, which are mostly oriented around a set of default or optional APIs exposed. For instance, a
workspace intended for use deploying Kube applications might expose the same API objects a user would encounter on a
physical cluster. A workspace intended for building functions might expose only the knative serving APIs, config maps
and secrets, and optionally enable knative eventing APIs.&lt;/p>
&lt;p>At the current time there is no decision on whether a workspace type represents an inheritance or composition model,
although in general we prefer composition approaches. We also do not have a fully resolved design.&lt;/p>
&lt;h2 id="virtual-workspace">Virtual Workspace&lt;/h2>
&lt;p>An API object has one source of truth (is stored transactionally in one system), but may be exposed to different use
cases with different fields or schemas. Since a workspace is the user facing interaction with an API object, if we want
to deal with Workspaces in aggregate, we need to be able to list them. Since a user may have access to workspaces in
multiple different contexts, or for different use cases (a workspace that belongs to the user personally, or one that
belongs to a business organization), the list of “all workspaces” itself needs to be exposed as an API object to an end
user inside a workspace. That workspace is “virtual” - it adapts or transforms the underlying source of truth for the
object and potentially the schema the user sees.&lt;/p>
&lt;h2 id="index-eg-workspace-index">Index (e.g. Workspace Index)&lt;/h2>
&lt;p>An index is the authoritative list of a particular API in their source of truth across the system. For instance, in
order for a user to see all the workspaces they have available, they must consult the workspace index to return a list
of their workspaces. It is expected that indices are suitable for consistent LIST/WATCHing (in the kubernetes sense) so
that integrations can be built to view the list of those objects.&lt;/p>
&lt;p>Index in the control plane sense should not be confused with secondary indices (in the database sense), which may be
used to enable a particular index.&lt;/p>
&lt;h2 id="shard">Shard&lt;/h2>
&lt;p>A failure domain within the larger control plane service that cuts across the primary functionality. Most distributed
systems must separate functionality across shards to mitigate failures, and typically users interact with shards through
some transparent serving infrastructure. Since the primary problem of building distributed systems is reasoning about
failure domains and dependencies across them, it is critical to allow operators to effectively match shards, understand
dependencies, and bring them together.&lt;/p>
&lt;p>A control plane should be shardable in a way that maximizes application SLO - gives users a tool that allows them to
better define their applications not to fail.&lt;/p>
&lt;h2 id="api-binding">API Binding&lt;/h2>
&lt;p>The act of associating a set of APIs with a given logical cluster. The Workspace model defines one particular
implementation of the lifecycle of a logical cluster and the APIs within it. Because APIs and the implementations that
back an API evolve over time, it is important that the binding be introspectable and orchestrate-able - that a consumer
can provide a rolling deployment of a new API or new implementation across hundreds or thousands of workspaces.&lt;/p>
&lt;p>There are likely a few objects involved in defining the APIs exposed within a workspace, but in general they probably
define a spec (which APIs / implementations to associate with) and a status (the chosen APIs / implementations that are
currently bound), allow a user to bulk associate APIs (i.e. multiple APIs at the same time, like “all knative serving
APIs”), and may be defaulted based on some attributes of a workspace type (all workspaces of this “type” get the default
Kube APIs, this other “type” get the knative apis).&lt;/p>
&lt;p>The evolution of an API within a workspace and across workspaces is of key importance.&lt;/p>
&lt;h2 id="syncer">Syncer&lt;/h2>
&lt;p>A syncer is installed on a SyncTarget and is responsible for synchronizing data between kcp and that cluster.&lt;/p>
&lt;h2 id="location">Location&lt;/h2>
&lt;p>A collection of SyncTargets that describe runtime characteristics that allow placement of applications.
Characteristics are not limited but could describe things like GPU, supported storage, compliance or
regulatory fulfillment, or geographical placement.&lt;/p></description></item><item><title>Docs: Virtual Workspaces</title><link>/docs/kcp-documenation/virtual-workspaces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/virtual-workspaces/</guid><description>
&lt;p>Virtual workspaces are proxy-like apiservers under a custom URL that provide some computed view of real workspaces.&lt;/p>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;ol>
&lt;li>when the user does &lt;code>kubectl get workspaces&lt;/code> only workspaces are shown that the user has access to. That resource is implemented through a virtual workspace under &lt;code>/services/workspaces/&amp;lt;org&amp;gt;/personal/apis/tenancy.kcp.dev/v1beta1/workspaces&lt;/code>.&lt;/li>
&lt;li>controllers should not be able to directly access customer workspaces. They should only be able to access the objects that are connected to their provided APIs. In &lt;a href="https://www.youtube.com/watch?v=Ca3vh3lS6YI&amp;amp;t=1280s">April 19&amp;rsquo;s community call this virtual workspace was showcased&lt;/a>, developed during v0.4 phase.&lt;/li>
&lt;li>if we keep the initializer model with &lt;code>ClusterWorkspaceTypes&lt;/code>, there must be a virtual workspace for the &amp;ldquo;workspace type owner&amp;rdquo; that gives access to initializing workspaces.&lt;/li>
&lt;li>the syncer will get a virtual workspace view of the workspaces it syncs to physical clusters. That view will have transformed objects potentially, especially deployment-splitter-like transformations will be implemented within a virtual workspace, transparently applied from the point of view of the syncer.&lt;/li>
&lt;/ol>
&lt;h2 id="faq">FAQ&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Can we use go clients to watch resources on a virtual workspace?&lt;/strong> Absolutely. From the point of view of the controllers it is just a normal (client) URL. So one can use client-go informers (or controller-runtime) to watch the objects in a virtual workspace.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> a normal service account lives in just ONE workspace and can only access its own workspace. So in order to use a service account for accessing cross-workspace data (and that&amp;rsquo;s what is necessary in example 2 and 3 at least), we need a virtual workspace to add the necessary authz.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Are virtual workspaces read-only?&lt;/strong> No, they are not necessarily. Some are, some are not. The controller view virtual workspace will be writable, as well as the syncer virtual workspace.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do service teams have to write their own virtual workspace?&lt;/strong> Not for the standard cases as described above. There might be cases in the future where service teams provide their own virtual workspace for some very special purpose access patterns. But we are not there yet.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Where does the developer get the URL from of the virtual workspace?&lt;/strong> The URLs will be &amp;ldquo;published&amp;rdquo; in some object status. E.g. APIExport.status will have a list of URLs that controllers have to connect to (example 2). Similarly, SyncTarget.status will have URLs for the syncer virtual workspaces, etc. We might do the same in ClusterWorkspaceType.status (example 3).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Will there be multiple virtual workspace URLs my controller has to watch?&lt;/strong> Yes, as soon as we add sharding, it will become a list. So it might be that 1000 tenants are accessible under one URL, the next 1000 under another one, and so on. The controllers have to watch the mentiond URL lists in status of objects and start new instances (either with their own controller sharding eventually, or just in process with another go routine).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Show me the code.&lt;/strong> The stock kcp virtual workspaces are in &lt;a href="../pkg/virtual">&lt;code>pkg/virtual&lt;/code>&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Who runs the virtual workspaces?&lt;/strong> The stock kcp virtual workspaces will be run through &lt;code>kcp start&lt;/code> in-process. The personal workspace one (example 1) can also be run as its own process and the kcp apiserver will forward traffic to the external address. There might be reasons in the future like scalability that the later model is preferred. For the clients of virtual workspaces that has no impact. They are supposed to &amp;ldquo;blindly&amp;rdquo; use the URLs published in the API objects&amp;rsquo; status. Those URLs might point to in-process instances or external addresses depending on deployment topology.&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Workspaces</title><link>/docs/kcp-documenation/workspaces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/workspaces/</guid><description>
&lt;p>Multi-tenancy is implemented through workspaces. A workspace is a Kubernetes-cluster-like
HTTPS endpoint, i.e. an endpoint usual Kubernetes client tooling (client-go, controller-runtime
and others) and user interfaces (kubectl, helm, web console, &amp;hellip;) can talk to like to a
Kubernetes cluster.&lt;/p>
&lt;p>Workspaces can be backed by a traditional REST store implementation through CRDs
or native resources persisted in etcd. But there can be alternative implementations
for special access patterns, e.g. a virtual workspace apiserver that transforms
other APIs e.g. by projections (Workspace in kcp is a projection of ClusterWorkspace)
or by applying visibility filters (e.g. showing all workspaces or all namespaces
the current user has access to).&lt;/p>
&lt;p>Workspaces are represented to the user via the Workspace kind, e.g.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Workspace&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">tenancy.kcp.dev/v1beta1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">Universal&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">status&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">url&lt;/span>: &lt;span style="color:#ae81ff">https://kcp.example.com/clusters/myapp&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is a 3-level hierarchy of workspaces:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Enduser Workspaces&lt;/strong> are workspaces holding enduser resources, e.g.
applications with services, secrets, configmaps, deployments, etc.&lt;/li>
&lt;li>&lt;strong>Organization Workspaces&lt;/strong> are workspaces holding organizational data,
e.g. definitions of enduser workspaces, roles, policies, accounting data.&lt;/li>
&lt;li>&lt;strong>Root Workspace&lt;/strong> is a singleton holding cross-organizational data and
the definition of the organizations.&lt;/li>
&lt;/ul>
&lt;h2 id="clusterworkspaces">ClusterWorkspaces&lt;/h2>
&lt;p>ClusterWorkspaces define traditional etcd-based, CRD enabled workspaces, available
under &lt;code>/clusters/&amp;lt;parent-workspace-name&amp;gt;:&amp;lt;cluster-workspace-name&amp;gt;&lt;/code>. E.g. organization
workspaces are accessible at &lt;code>/clusters/root:&amp;lt;org-name&amp;gt;&lt;/code>. An enduser workspace is
accessible at &lt;code>/clusters/&amp;lt;org-name&amp;gt;:&amp;lt;enduser-workspace-name&amp;gt;&lt;/code>.&lt;/p>
&lt;p>ClusterWorkspaces have a type. A type is defined by a ClusterWorkspaceType. A type
defines initializers. They are set on new ClusterWorkspace objects and block the
cluster workspace from leaving the initializing phase. Both system components and
3rd party components can use initializers to customize ClusterWorkspaces on creation,
e.g. to bootstrap resources inside the workspace, or to set up permission in its parent.&lt;/p>
&lt;p>A cluster workspace of type &lt;code>Universal&lt;/code> is a workspace without further initialization
or special properties by default, and it can be used without a corresponding
ClusterWorkspaceType object (though one can be added and its initializers will be
applied). ClusterWorkSpaces of type &lt;code>Organization&lt;/code> are described in the next section.&lt;/p>
&lt;p>Note: in order to create cluster workspaces of a given type (including &lt;code>Universal&lt;/code>)
you must have &lt;code>use&lt;/code> permissions against the &lt;code>clusterworkspacetypes&lt;/code> resources with the
lower-case name of the cluster workspace type (e.g. &lt;code>universal&lt;/code>). All &lt;code>system:authenticated&lt;/code>
users inherit this permission automatically for type &lt;code>Universal&lt;/code>.&lt;/p>
&lt;p>ClusterWorkspaces persisted in etcd on a shard have disjoint etcd prefix ranges, i.e.
they have independent behaviour and no cluster workspace sees objects from other
cluster workspaces. In contrast to namespace in Kubernetes, this includes non-namespaced
objects, e.g. like CRDs where each workspace can have its own set of CRDs installed.&lt;/p>
&lt;h2 id="user-home-workspaces">User Home Workspaces&lt;/h2>
&lt;p>User home workspaces are an optional feature of kcp. If enabled (through &lt;code>--enable-home-workspaces&lt;/code>), there is a special
virtual &lt;code>Workspace&lt;/code> called &lt;code>~&lt;/code> in the root workspace. It is used by &lt;code>kubectl ws&lt;/code> to derive the full path to the user
home workspace, similar to how Unix &lt;code>cd ~&lt;/code> move the users to their home.&lt;/p>
&lt;p>The full path for a user&amp;rsquo;s home workspace has a number of parts: &lt;code>&amp;lt;prefix&amp;gt;:(&amp;lt;bucket&amp;gt;)+&amp;lt;user-name&amp;gt;&lt;/code>. Buckets are used to
ensure that ~1000 sub-buckets or users exist in any bucket, for scaling reasons. The bucket names are deterministically
derived from the user name (via some hash). Example for user &lt;code>adam&lt;/code> when using default configuration:
&lt;code>root:users:a8:f1:adam&lt;/code>.&lt;/p>
&lt;p>User home workspaces are created on-demand when they are first accessed, but this is not visible to the user, allowing
the system to only incur the cost of these workspaces when they are needed. Only users of the configured
home-creator-groups (default &lt;code>system:authenticated&lt;/code>) will have a home workspace.&lt;/p>
&lt;p>The following cluster workspace types are created internally to support User Home Workspaces:&lt;/p>
&lt;ul>
&lt;li>&lt;code>homeroot&lt;/code>: the workspace that will contain all the Home workspaces, spread accross buckets. - Can contain only Homebucket workspaces&lt;/li>
&lt;li>&lt;code>homebucket&lt;/code>: the type of workspaces that will contain a subset of all home workspaces. - Can contain either Homebucket (multi-level bucketing) or Home workspaces&lt;/li>
&lt;li>&lt;code>home&lt;/code>: the ClusterWorkspace of home workspaces - can contain any type of workspaces as children (especially universal workspaces)&lt;/li>
&lt;/ul>
&lt;h3 id="bucket-configuration-options">Bucket configuration options&lt;/h3>
&lt;p>The &lt;code>kcp&lt;/code> administrator can configure:&lt;/p>
&lt;ul>
&lt;li>&lt;code>&amp;lt;prefix&amp;gt;&lt;/code>, which defaults to &lt;code>root:users&lt;/code>&lt;/li>
&lt;li>bucket depth, which defaults to 2&lt;/li>
&lt;li>bucket name length, in characters, which defaults to 2&lt;/li>
&lt;/ul>
&lt;p>The following outlines valid configuration options. With the default setup, ~5 users or ~700 sub-buckets will be in
any bucket.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>NOTE&lt;/strong>: DO NOT set the bucket size to be longer than 2, as this will adversely impact performance.&lt;/p>
&lt;/blockquote>
&lt;p>User-names have &lt;code>(26 * [(26 + 10 + 2) * 61] * 36 = 2169648)&lt;/code> permutations, and buckets are made up of lowercase-alpha
chars. Invalid configurations break the scale limit in sub-buckets or users. Valid configurations should target
having not more than ~1000 sub-buckets per bucket and at least 5 users per bucket.&lt;/p>
&lt;p>&lt;strong>Valid Configurations&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>length&lt;/th>
&lt;th>depth&lt;/th>
&lt;th>sub-buckets&lt;/th>
&lt;th>users&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>3&lt;/td>
&lt;td>26 * 1 = 26&lt;/td>
&lt;td>2169648 / (26)^3 = 124&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>4&lt;/td>
&lt;td>26 * 1 = 26&lt;/td>
&lt;td>2169648 / (26)^4 = 5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>26 * 26 = 676&lt;/td>
&lt;td>2169648 / (26*26)^2 = 5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Invalid Configurations&lt;/strong>&lt;/p>
&lt;p>These are examples of invalid configurations and are for illustrative purposes only. In nearly all cases, the default values
will be sufficient.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>length&lt;/th>
&lt;th>depth&lt;/th>
&lt;th>sub-buckets&lt;/th>
&lt;th>users&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>26 * 1 = 26&lt;/td>
&lt;td>2169648 / (26) = 83448&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>26 * 1 = 26&lt;/td>
&lt;td>2169648 / (26)^2 = 3209&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>26 * 26 = 676&lt;/td>
&lt;td>2169648 / (26*26) = 3209&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;td>26 * 26 = 676&lt;/td>
&lt;td>2169648 / (26*26)^3 = .007&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>26 &lt;em>26&lt;/em> 26 = 17576&lt;/td>
&lt;td>2169648 / (26&lt;em>26&lt;/em>26) = 124&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2&lt;/td>
&lt;td>26 &lt;em>26&lt;/em> 26 = 17576&lt;/td>
&lt;td>2169648 / (26&lt;em>26&lt;/em>26)^2 = .007&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="organization-workspaces">Organization Workspaces&lt;/h2>
&lt;p>Organization workspaces are ClusterWorkspaces of type &lt;code>Organization&lt;/code>, defined in the
root workspace. Organization workspaces are accessible at &lt;code>/clusters/root:&amp;lt;org-name&amp;gt;&lt;/code>.&lt;/p>
&lt;p>Note: the organization ClusterWorkspaceType can only be created in the root workspace
verified through admission.&lt;/p>
&lt;p>Organization workspaces have standard resources (on-top of &lt;code>Universal&lt;/code> workspaces)
which include the &lt;code>ClusterWorkspace&lt;/code> API defined through an CRD deployed during
organization workspace initialization.&lt;/p>
&lt;h2 id="root-workspace">Root Workspace&lt;/h2>
&lt;p>The root workspace is a singleton in the system accessible under &lt;code>/clusters/root&lt;/code>.
It is not represented by a ClusterWorkspace anywhere, but shares the same properties.&lt;/p>
&lt;p>Inside the root workspace at least the following resources are bootstrapped on
kcp startup:&lt;/p>
&lt;ul>
&lt;li>ClusterWorkspace CRD&lt;/li>
&lt;li>WorkspaceShard CRD&lt;/li>
&lt;li>Cluster CRD.&lt;/li>
&lt;/ul>
&lt;p>The root workspace is the only one that holds WorkspaceShard objects. WorkspaceShards
are used to schedule a new ClusterWorkspace to, i.e. to select in which etcd the
cluster workspace content is to be persisted.&lt;/p>
&lt;h2 id="system-workspaces">System Workspaces&lt;/h2>
&lt;p>System workspaces are local to a shard and are named in the pattern &lt;code>system:&amp;lt;system-workspace-name&amp;gt;&lt;/code>.&lt;/p>
&lt;p>They are only accessible to a shard-local admin user and there is neither a definition
via a ClusterWorkspace nor any per-request check for workspace existence.&lt;/p>
&lt;p>System workspace are only accessible to a shard-local admin user, and there is
neither a definition via a ClusterWorkspace, nor is there any validation of requests
that the system workspace exists.&lt;/p>
&lt;p>The &lt;code>system:admin&lt;/code> system workspace is special as it is also accessible through &lt;code>/&lt;/code>
of the shard, and at &lt;code>/cluster/system:admin&lt;/code> at the same time.&lt;/p></description></item><item><title>Docs: Logical Clusters</title><link>/docs/kcp-documenation/investigations/logical-clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/investigations/logical-clusters/</guid><description>
&lt;p>Kubernetes evolved from and was influenced by earlier systems that had weaker internal tenancy than a general-purpose compute platform requires. The namespace, quota, admission, and RBAC concepts were all envisioned quite early in the project, but evolved with Kubernetes and not all impacts to future evolution were completely anticipated. Tenancy within clusters is handled at the resource and namespace level, and within a namespace there are a limited number of boundaries. Most organizations use either namespace or cluster separation as their primary unit of self service, with variations leveraging a rich ecosystem of tools.&lt;/p>
&lt;p>The one concrete component that cannot be tenanted are API resources - from a Kubernetes perspective we have too much history and ecosystem to desire a change, and so intra-cluster tenancy will always be at its weakest when users desire to add diverse extensions and tools. In addition, controllers are practically limited to a cluster scope or a namespace scope by the design of our APIs and authorization models and so intra-cluster tenancy for extensions is particularly weak (you can&amp;rsquo;t prevent an ingress controller from viewing &amp;ldquo;all secrets&amp;rdquo;).&lt;/p>
&lt;p>Our admission chain has historically been very powerful and allowed deep policy to be enforced for tenancy, but the lack of a dynamic plugin model in golang has limited us to what can be accomplished to external RPC webhooks which have a number of significant performance and reliability impacts (especially when coupled to the cluster the webhook is acting on). If we want to have larger numbers of tenants or stronger subdivision, we need to consider improving the scalability of our policy chain in a number of dimensions.&lt;/p>
&lt;p>Ideally, as a community we could improve both namespace and cluster tenancy at the same time in a way that provides enhanced tools for teams and organizations, addresses extensions holistically, and improves the reliability and performance of policy control from our control planes.&lt;/p>
&lt;h2 id="goal-getting-a-new-cluster-that-allows-a-team-to-add-extensions-efficiently-should-be-effectively-zero-cost">Goal: Getting a new cluster that allows a team to add extensions efficiently should be effectively zero cost&lt;/h2>
&lt;p>If a cluster is a desirable unit of tenancy, clusters should be amortized &amp;ldquo;free&amp;rdquo; and easy to operationalize as self-service. We have explored in the community a number of approaches that make new clusters cheaper (specifically &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster">virtual clusters&lt;/a> in SIG-multi-tenancy, as well as the natural cloud vendor &amp;ldquo;as-a-service&amp;rdquo; options where they amortize the cost of many small clusters), but there are certain fundamental fixed costs that inflate the cost of those clusters. If we could make one more cluster the same cost as a namespace, we could dramatically improve isolation of teams as well as offering an advantage for more alignment on tenancy across the ecosystem.&lt;/p>
&lt;h3 id="constraint-a-naive-client-should-see-no-difference-between-a-physical-or-logical-cluster">Constraint: A naive client should see no difference between a physical or logical cluster&lt;/h3>
&lt;p>A logical cluster that behaves differently from a physical cluster is not valuable for existing tools. We would need to lean heavily on our existing abstractions to ensure clients see no difference, and to focus on implementation options that avoid reduplicating a large amount of the Kube API surface area.&lt;/p>
&lt;h3 id="constraint-the-implementation-must-improve-isolation-within-a-single-process">Constraint: The implementation must improve isolation within a single process&lt;/h3>
&lt;p>As clusters grow larger or are used in more complex fashion, the failure modes of a single process API server have received significant attention within the last few years. To offer cheaper clusters, we&amp;rsquo;d have to also improve isolation between simultaneous clients and manage etcd usage, traffic, and both cpu and memory use at the control plane. These stronger controls would be beneficial to physical clusters and organizations running more complex clusters as well.&lt;/p>
&lt;h3 id="constraint-we-should-improve-in-process-options-for-policy">Constraint: We should improve in-process options for policy&lt;/h3>
&lt;p>Early in Kubernetes we discussed our options for extension of the core capability - admission control and controllers are the two primary levers, with aggregated APIs being an escape hatch for more complex API behavior (including the ability to wrap existing APIs or CRDs). We should consider options that could reduce the cost of complex policy such as making using the Kube API more library-like (to enable forking) as well as in-process options for policy that could deliver order of magnitude higher reliability and performance than webhooks.&lt;/p>
&lt;h2 id="areas-of-investigation">Areas of investigation&lt;/h2>
&lt;ol>
&lt;li>Define high level user use cases&lt;/li>
&lt;li>Prototype modelling the simplest option to enable &lt;code>kcp&lt;/code> demo functionality and team subdivision&lt;/li>
&lt;li>Explore client changes required to make multi-cluster controllers efficient&lt;/li>
&lt;li>Support surfacing into a logical cluster API resources from another logical cluster&lt;/li>
&lt;li>Layering RBAC so that changes in one logical cluster are additive to a source policy that is out of the logical cluster&amp;rsquo;s control&lt;/li>
&lt;li>Explore quota of requests, cpu, memory, and persistent to complement P&amp;amp;F per logical cluster with hard and soft limits&lt;/li>
&lt;li>Explore making &lt;code>kcp&lt;/code> usable as a library so that an extender could write Golang admission / hierarchal policy for logical clusters that reduces the need for external extension&lt;/li>
&lt;li>Work through how a set of etcd objects could be moved to another etcd for sharding operations and keep clients unaware (similar to the &amp;ldquo;restore a cluster from backup&amp;rdquo; problem)&lt;/li>
&lt;li>Explore providing a read only resource underlay from another logical cluster so that immutable default objects can be provided&lt;/li>
&lt;li>Investigate use cases that would benefit even a single cluster (justify having this be a feature in kube-apiserver on by default)&lt;/li>
&lt;/ol>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>The use cases of logical cluster can be seen to overlap heavily with &lt;a href="transparent-multi-cluster.md">transparent multi-cluster&lt;/a> use cases, and are captured at the highest level in &lt;a href="../../GOALS.md">GOALS.md&lt;/a>. The use cases below attempt to focus on logical clusters independent of the broader goals.&lt;/p>
&lt;h3 id="as-a-developer-of-crds--controllers--extensions">As a developer of CRDs / controllers / extensions&lt;/h3>
&lt;ul>
&lt;li>I can launch a local Kube control plane and test out multiple different versions of the same CRD in parallel quickly&lt;/li>
&lt;li>I can create a control plane for my organization&amp;rsquo;s cloud resources (CRDs) that is centralized but doesn&amp;rsquo;t require me to provision nodes.&lt;/li>
&lt;/ul>
&lt;h3 id="as-an-infrastructure-admin">As an infrastructure admin&lt;/h3>
&lt;ul>
&lt;li>I can have strong tenant separation between different application teams&lt;/li>
&lt;li>Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others&lt;/li>
&lt;li>Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure&lt;/li>
&lt;/ul>
&lt;h3 id="as-a-user-on-an-existing-kubernetes-cluster">As a user on an existing Kubernetes cluster&lt;/h3>
&lt;ul>
&lt;li>I can get a temporary space to test an extension before installing it&lt;/li>
&lt;li>I can create clusters that have my own namespaces&lt;/li>
&lt;/ul>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;h3 id="logical-clusters-represented-as-a-prefix-to-etcd">Logical clusters represented as a prefix to etcd&lt;/h3>
&lt;p>In the early prototype stage &lt;code>kcp&lt;/code> has a series of patches that allow a header or API prefix path to alter the prefix used to retrieve resources from etcd. The set of available resources is stripped down to a minimal set of hardcoded APIs including namespaces, rbac, and crds by patching those out of kube-apiserver type registration.&lt;/p>
&lt;p>The header &lt;code>X-Kubernetes-Cluster&lt;/code> supports either a named logical cluster or the value &lt;code>*&lt;/code>, or the prefix &lt;code>/cluster/&amp;lt;name&amp;gt;&lt;/code> may be used at the root. This alters the behavior of a number of components, primarily retrieval and storage of API objects in etcd by adding a new segment to the etcd key (instead of &lt;code>/&amp;lt;resource&amp;gt;/&amp;lt;namespace&amp;gt;/&amp;lt;name&amp;gt;&lt;/code>, &lt;code>/&amp;lt;resource&amp;gt;/&amp;lt;cluster&amp;gt;/&amp;lt;namespace&amp;gt;/&amp;lt;name&amp;gt;&lt;/code>). Providing &lt;code>*&lt;/code> is currently acting on watch to support watching resources across all clusters, which also has the side effect of populating the object &lt;code>metadata.clusterName&lt;/code> field. If no logical cluster name is provided, the value &lt;code>admin&lt;/code> is used (which behaves as a normal kube-apiserver would).&lt;/p>
&lt;p>This means new logical clusters start off empty (no RBAC or CRD resources), which the &lt;code>kcp&lt;/code> prototype mitigates by calculating the set of API resources available by merging from the default &lt;code>admin&lt;/code> CRDs + the hardcoded APIs. That demonstrates one avenue of efficiency - a new logical cluster has an amortized cost near zero for both RBAC (no duplication of several hundred RBAC roles into the logical cluster) and API OpenAPI documents (built on demand as the union of another logical cluster and any CRDs added to the new cluster).&lt;/p>
&lt;p>To LIST or WATCH these resources, the user specifies &lt;code>*&lt;/code> as their cluster name which adjusts the key prefix to fetch all resources across all logical clusters. It&amp;rsquo;s likely some intermediate step between client and server would be necessary to support &amp;ldquo;watch subset&amp;rdquo; efficiently, which would require work to better enable clients to recognize the need to relist as well as the need to make the prototype support some level of logical cluster subset retrieval besides just an etcd key prefix scan.&lt;/p>
&lt;h4 id="next-steps">Next steps&lt;/h4>
&lt;ul>
&lt;li>Continuing to explore how clients might query multiple resources across multiple logical clusters&lt;/li>
&lt;li>What changes to resource version are necessary to allow&lt;/li>
&lt;/ul>
&lt;h3 id="zero-configuration-on-startup-in-local-dev">Zero configuration on startup in local dev&lt;/h3>
&lt;p>The &lt;code>kcp&lt;/code> binary embeds &lt;code>etcd&lt;/code> in a single node config and manages it for local iterative development. This is in keeping with optimize for local workflow, but can be replaced by connecting to an existing etcd instance (not currently implemented). Ideally, a &lt;code>kcp&lt;/code> like process would have minimal external dependencies and be capable of running in a shardable configuration efficiently (each shard handling 100k objects), with other components handling logical cluster sharding.&lt;/p>
&lt;h3 id="crd-virtualization-inheritance-and-normalization">CRD virtualization, inheritance, and normalization&lt;/h3>
&lt;p>A simple implementation of CRD virtualization (different logical clusters having different api resources), CRD inheritance (a logical cluster inheriting CRDs from a parent logical cluster), and CRD normalization (between multiple physical clusters) to find the lowest-common-denominator resource has been prototyped.&lt;/p>
&lt;p>The CRD structure in the kube-apiserver is currently &amp;ldquo;up front&amp;rdquo; (as soon as a CRD is created it shows up in apiresources), but with the goal of reducing the up front cost of a logical cluster we may wish to suggest refactors upstream that would make the model more amenable to &amp;ldquo;on demand&amp;rdquo; construction and merging at runtime. OpenAPI merging is a very expensive part of the kube-apiserver historically (rapid CRD changes can have a massive memory and CPU impact) and this may be a logical area to invest to allow scaling within regular clusters.&lt;/p>
&lt;p>Inheritance allows an admin to control which resources a client might use - this would be particularly useful in more opinionated platform flows for organizations that wish to offer only a subset of APIs. The simplest approach here is that all logical clusters inherit the admin virtual cluster (the default), but more complicated flows with policy and chaining should be possible.&lt;/p>
&lt;p>Normalization involves reading OpenAPI docs from one or more child clusters, converting those to CRDs, finding the lowest compatible version of those CRDs (the version that shares all fields), and materializing those objects as CRDs in a logical cluster. This allows the minimum viable hook for turning a generic control plane into a spot where real Kube objects can run, and would be a key part of transparent multi-cluster.&lt;/p></description></item><item><title>Docs: Minimal API Server</title><link>/docs/kcp-documenation/investigations/minimal-api-server/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/investigations/minimal-api-server/</guid><description>
&lt;p>The Kubernetes API machinery provides a pattern for declarative config-driven API with a number of conventions that simplify building configuration loops and consolidating sources of truth. There have been many efforts to make that tooling more reusable and less dependent on the rest of the Kube concepts but without a strong use case driving separation and a design the tooling is still fairly coupled to Kube.&lt;/p>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>Building and sustaining an API server that:&lt;/p>
&lt;ol>
&lt;li>reuses much of the Kubernetes API server codebase to support Kube-like CRUD operations&lt;/li>
&lt;li>adds, removes, or excludes some / any / all the built-in Kubernetes types&lt;/li>
&lt;li>excludes some default assumptions of Kubernetes specific to the &amp;ldquo;Kube as a cluster&amp;rdquo; like &amp;ldquo;create the kubernetes default svc&amp;rdquo;&lt;/li>
&lt;li>replaces / modifies some implementations like custom resources, backend storage (etcd vs others), RBAC, admission control, and other primitives&lt;/li>
&lt;/ol>
&lt;p>As a secondary goal, identifying where exceptions or undocumented assumptions exist in the libraries that would make clients behave differently generically (where an abstraction is not complete) should help ensure future clients can more concretely work across different API servers consistently.&lt;/p>
&lt;h3 id="constraint-the-abstraction-for-a-minimal-api-server-should-not-hinder-kubernetes-development">Constraint: The abstraction for a minimal API server should not hinder Kubernetes development&lt;/h3>
&lt;p>The primary consumer of the Kube API is Kubernetes - any abstraction that makes a standalone API server possible must not regress Kubernetes performance or overly complicate Kubernetes evolution. The abstraction &lt;em>should&lt;/em> be an opportunity to improve interfaces within Kubernetes to decouple components and improve comprehension.&lt;/p>
&lt;h3 id="constraint-reusing-the-existing-code-base">Constraint: Reusing the existing code base&lt;/h3>
&lt;p>While it is certainly possible to rebuild all of Kube from scratch, a large amount of client tooling and benefit exists within patterns like declarative apply. This investigation is scoped to working within the context of improving the existing code and making the minimal changes within the bounds of API compatibility to broaden utility.&lt;/p>
&lt;p>It should be possible to add an arbitrary set of the existing Kube resources to the minimal API server, up to and including what an existing kube-apiserver exposes. Several use cases desire RBAC, Namespaces, Secrets, or other parts of the workload, while ensuring a &amp;ldquo;pick and choose&amp;rdquo; mindset keeps the interface supporting the needs of the full Kubernetes server.&lt;/p>
&lt;p>In the short term, it would not be a goal of this investigation to replace the underlying storage implementation &lt;code>etcd&lt;/code>, but it should be possible to more easily inject the appropriate initialization code so that someone can easily start an API server that uses a different storage mechanism.&lt;/p>
&lt;h2 id="areas-of-investigation">Areas of investigation&lt;/h2>
&lt;ol>
&lt;li>Define high level user use cases&lt;/li>
&lt;li>Document existing efforts inside and outside of the SIG process&lt;/li>
&lt;li>Identify near-term SIG API-Machinery work that would benefit from additional decoupling (also wg-code-organization)&lt;/li>
&lt;li>Find consensus points on near term changes and draft a KEP&lt;/li>
&lt;/ol>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;h3 id="as-a-developer-of-crds--controllers--extensions">As a developer of CRDs / controllers / extensions&lt;/h3>
&lt;ul>
&lt;li>I can launch a local Kube API and test out multiple different versions of the same CRD in parallel quickly (shared with &lt;a href="logical-clusters.md">logical-clusters&lt;/a>)&lt;/li>
&lt;li>I can create a control plane for my organization&amp;rsquo;s cloud resources (CRDs) that is centralized but doesn&amp;rsquo;t require me to provision nodes (shared with &lt;a href="logical-clusters.md">logical-clusters&lt;/a>)&lt;/li>
&lt;li>&amp;hellip; benefits for unit testing CRDs in controller projects?&lt;/li>
&lt;/ul>
&lt;h3 id="as-a-kubernetes-core-developer">As a Kubernetes core developer&lt;/h3>
&lt;ul>
&lt;li>The core API server libraries are better separated and more strongly reviewed&lt;/li>
&lt;li>Additional contributors are incentivized to maintain the core libraries of Kube because of a broader set of use cases&lt;/li>
&lt;li>Kube client tools have fewer edge cases because they are tested against multiple sets of resources&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="as-an-aggregated-api-server-developer">As an aggregated API server developer&lt;/h3>
&lt;ul>
&lt;li>It is easy to reuse the k8s.io/apiserver code base to provide:
&lt;ul>
&lt;li>A virtual read-only resource that proxies to another type
&lt;ul>
&lt;li>e.g. metrics-server&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>An end user facing resource backed by a CRD (editable only by admins) that has additional validation and transformation
&lt;ul>
&lt;li>e.g. service catalog&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A subresource implementation for a core type (pod/logs) that is not embedded in the Kube apiserver code&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="as-a-devops-team">As a devops team&lt;/h3>
&lt;ul>
&lt;li>I want to be able to create declarative APIs using the controller pattern &amp;hellip;
&lt;ul>
&lt;li>So that I can have declarative infrastructure without a full Kube cluster (&lt;a href="https://github.com/thetirefire/badidea">https://github.com/thetirefire/badidea&lt;/a> and &lt;a href="https://docs.google.com/presentation/d/1TfCrsBEgvyOQ1MGC7jBKTvyaelAYCZzl3udRjPlVmWg/edit#slide=id.g401c104a3c_0_0">https://docs.google.com/presentation/d/1TfCrsBEgvyOQ1MGC7jBKTvyaelAYCZzl3udRjPlVmWg/edit#slide=id.g401c104a3c_0_0&lt;/a>)&lt;/li>
&lt;li>So that I can have controllers that list/watch/sync/react to user focused changes&lt;/li>
&lt;li>So that I can have a kubectl apply loop for my intent (spec) and see the current state (status)&lt;/li>
&lt;li>So that I can move cloud infrastructure integrations like &lt;a href="https://github.com/aws-controllers-k8s/community">AWS Controllers for k8s&lt;/a> out of individual clusters into a centrally secured spot&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>I want to be offer a &amp;ldquo;cluster-like&amp;rdquo; user experience to a Kube application author without exposing the cluster directly (&lt;a href="transparent-multi-cluster.md">transparent multi-cluster&lt;/a>)
&lt;ul>
&lt;li>So that I can keep app authors from directly knowing about where the app runs for security / infrastructure abstraction&lt;/li>
&lt;li>So that I can control where applications run across multiple clusters centrally&lt;/li>
&lt;li>So that I can offer self-service provisioning at a higher level than namespace or cluster&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>I want to consolidate all of my infrastructure and use gitops to talk to them the same way I do for clusters
&lt;ul>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>More detailed requests&lt;/p>
&lt;ul>
&lt;li>With some moderate boilerplate (50-100 lines of code) I can start a Kube compliant API server with (some / any of):
&lt;ul>
&lt;li>Only custom built-in types (code -&amp;gt; generic registry -&amp;gt; etcd)&lt;/li>
&lt;li>CRDs (CustomResourceDefinition)&lt;/li>
&lt;li>Aggregated API support (APIService)&lt;/li>
&lt;li>Quotas and rate control and priority and fairness&lt;/li>
&lt;li>A custom admission chain that does not depend on webhooks but is inline code&lt;/li>
&lt;li>A different backend for storage other than etcd (projects like &lt;a href="https://github.com/k3s-io/kine">kine&lt;/a>)&lt;/li>
&lt;li>Add / wrap some HTTP handlers with middleware&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;p>Initial work in k/k fork involved stripping out elements of kube-apiserver start that required &amp;ldquo;the full stack&amp;rdquo; or internal controllers such as the kubernetes.default.svc maintainer (roughly &lt;code>pkg/master&lt;/code>). It also looked at how to pull a subset of Kube resources (namespaces, rbac, but not pods) from the core resource group. The &lt;code>kcp&lt;/code> binary uses fairly normal &lt;code>k8s.io/apiserver&lt;/code> methods to init the apiserver process.&lt;/p>
&lt;p>Next steps include identifying example use cases and the interfaces they wish to customize in the control plane (see above) and then looking at how those could be composed in an approachable way. That also involves exploring what refactors and organization makes sense within the k/k project in concert with 1-2 sig-apimachinery members.&lt;/p></description></item><item><title>Docs: Self-service policy</title><link>/docs/kcp-documenation/investigations/self-service-policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/investigations/self-service-policy/</guid><description>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>Improve consistency and reusability of self-service and policy enforcement across multiple Kubernetes clusters.&lt;/p>
&lt;blockquote>
&lt;p>Just like Kubernetes standardized deploying containerized software onto a small set of machines, we want to standardize self-service of application focused integration across multiple teams with organizational control.&lt;/p>
&lt;/blockquote>
&lt;p>Or possibly&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes standardized deploying applications into chunks of capacity. We want to standardize isolating and integrating application teams across organizations, and to do that in a way that makes applications everywhere more secure.&lt;/p>
&lt;/blockquote>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>A key component of large Kubernetes clusters is shared use, where the usage pattern might vary from externally controlled (via gitops / existing operational tools) to a permissive self-service model. The most common partitioning model in Kubernetes is namespace, and the second most common model is cluster.&lt;/p>
&lt;p>Self-service is currently limited by the set of resources that are namespace scoped for the former, and by the need to parameterize and configure multiple clusters consistently for the latter. Cluster partitioning can uniquely offer distinct sets of APIs to consumers. Namespace partitioning is cheap up until the scale limits of the cluster (~10k namespaces), while cluster partitioning usually has a fixed cost per cluster in operational and resource usage, as well as lower total utilization.&lt;/p>
&lt;p>Once a deployment reaches the scale limit of a single cluster, operators often need to redefine their policies and tools to work in a multi-cluster environment. Many large deployers create their own systems for managing self-service policy above their clusters and leverage individual subsystems within Kubernetes to accomplish those goals.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>The logical cluster concept offers an opportunity to allow self-service at a cluster scope, with the effective cost of the namespace partitioning scheme. In addition, the separation of workload at control plane (kcp) and data plane (physical cluster) via &lt;a href="./transparent-multi-cluster.md">transparent multi-cluster&lt;/a> or similar schemes allows strong policy control of what configuration is allowed (reject early), restriction of the supported API surface area for workload APIs (limit / control certain fields like pod security), and limits the access of individual users to the underlying infra (much like clusters limit access to nodes).&lt;/p>
&lt;p>It should be possible to accomplish current self-service namespace and cluster partitioning via the logical cluster mechanism + policy enforcement, and to incentivize a wider range of &amp;ldquo;external policy control&amp;rdquo; users to adopt self-service via stronger control points and desirable use cases (multi-cluster resiliency for apps).&lt;/p>
&lt;p>We want to enable concrete points of injection of policy that are difficult today in Kubernetes tenancy:&lt;/p>
&lt;ol>
&lt;li>The acquisition of a new logical cluster with &lt;strong>capabilities&lt;/strong> and &lt;strong>constraints&lt;/strong>&lt;/li>
&lt;li>How the APIs in a logical cluster are &lt;strong>transformed&lt;/strong> to an underlying cluster&lt;/li>
&lt;li>How to manage the evolution of APIs available to a logical cluster over time&lt;/li>
&lt;li>New hierarchal policy options are more practical since different logical clusters can have different APIs&lt;/li>
&lt;/ol>
&lt;h2 id="areas-of-investigation">Areas of investigation&lt;/h2>
&lt;ul>
&lt;li>Using logical clusters as a mechanism for tenancy, but having a backing implementation that can change
&lt;ul>
&lt;li>I.e. materialize logical clusters as an API resource in a separate logical cluster&lt;/li>
&lt;li>Or implementing logical clusters outside the system and having the kcp server implementation be a shim&lt;/li>
&lt;li>Formal &amp;ldquo;policy module&amp;rdquo; implementations that can be plugged into a minimal API server while using logical cluster impl&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Catalog the set of tenancy constructs in use in Kube
&lt;ul>
&lt;li>Draw heavily on sig-multitenancy explorations - work done by &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster">cluster api nested&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster">virtual clusters&lt;/a>, namespace tenancy, and &lt;a href="https://github.com/kubernetes-sigs/hierarchical-namespaces">hierarchal namespace&lt;/a> designs&lt;/li>
&lt;li>Look at reference materials created by large organizational adopters of Kube&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Consider making &amp;ldquo;cost&amp;rdquo; a first class control concept alongside quota and RBAC (i.e. a service load balancer &amp;ldquo;costs&amp;rdquo; $1, whereas a regular service costs $0.001)
&lt;ul>
&lt;li>Could this more effectively limit user action&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Explore hierarchy of policy - if logical clusters are selectable by label, could you have composability of policy using controllers&lt;/li>
&lt;li>Explore using implicit resources
&lt;ul>
&lt;li>i.e. within a logical cluster have all resources of type RoleBinding be fetched from two sources - within the cluster, and in a separate logical cluster - and merged, so that you could change the global source and watches would still fire&lt;/li>
&lt;li>Impliict resources have risk though - no way to &amp;ldquo;lock&amp;rdquo; them so the consequences of an implicit change can be expensive&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;h3 id="simple-example-of-a-policy-implementation">Simple example of a policy implementation&lt;/h3>
&lt;p>Building out an example flow that goes from creating a logical cluster resource that results in a logical cluster being accessible to client, with potential hook points for deeper integration.&lt;/p>
&lt;h3 id="describe-a-complicated-policy-implementation">Describe a complicated policy implementation&lt;/h3>
&lt;p>An example hosted multi-tenant service with billing, organizational policy, and tenancy isolation.&lt;/p></description></item><item><title>Docs: Transparent multi-cluster</title><link>/docs/kcp-documenation/investigations/transparent-multi-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/kcp-documenation/investigations/transparent-multi-cluster/</guid><description>
&lt;p>A key tenet of Kubernetes is that workload placement is node-agnostic until the user needs it to be - Kube offers a homogeneous compute surface that admins or app devs can &amp;ldquo;break-glass&amp;rdquo; and set constraints all the way down to writing software that deeply integrates with nodes. But for the majority of workloads a cluster is no more important than a node - it&amp;rsquo;s a detail determined by some human or automated process.&lt;/p>
&lt;p>A key area of investigation for &lt;code>kcp&lt;/code> is exploring transparency of workloads to clusters. Aspirationally we want Kube workloads to be resilient to the operational characteristics of the underlying infrastructure and clusters orthogonally to the workload, by isolating the user from knowing of the details of the infrastructure. If workload APIs are more consistently &amp;ldquo;node-less&amp;rdquo; and &amp;ldquo;cluster-agnostic&amp;rdquo; that opens up ways to drive workload consistency across a large swathe of the compute landscape.&lt;/p>
&lt;h2 id="goal-the-majority-of-applications-and-teams-should-have-workflows-where-cluster-is-a-detail">Goal: The majority of applications and teams should have workflows where cluster is a detail&lt;/h2>
&lt;p>A number of projects have explored this since the beginning of Kubernetes - this prototype should explore in detail how we can make a normal Kubernetes flow for most users be cluster-independent but still &amp;ldquo;break-glass&amp;rdquo; and describe placement in detail. Since this is a broad topic and we want to benefit the majority of users, we need to also add constraints that maximize the chance of these approaches being adopted.&lt;/p>
&lt;h3 id="constraint-the-workflows-and-practices-teams-use-today-should-be-minimally-disrupted">Constraint: The workflows and practices teams use today should be minimally disrupted&lt;/h3>
&lt;p>Users typically only change their workflows when an improvement offers a significant multiplier. To be effective we must reduce friction (which reduces multipliers) and offer significant advantages to that workflow.&lt;/p>
&lt;p>Tools, practices, user experiences, and automation should &amp;ldquo;just work&amp;rdquo; when applied to cluster-agnostic or cluster-aware workloads. This includes gitops, rich web interfaces, &lt;code>kubectl&lt;/code>, etc. That implies that a &amp;ldquo;cluster&amp;rdquo; and a &amp;ldquo;Kube API&amp;rdquo; is our key target, and that we must preserve a majority of semantic meaning of existing APIs.&lt;/p>
&lt;h3 id="constraint-95-of-workloads-should-just-work-when-kubectl-applyd-to-kcp">Constraint: 95% of workloads should &amp;ldquo;just work&amp;rdquo; when &lt;code>kubectl apply&lt;/code>d to &lt;code>kcp&lt;/code>&lt;/h3>
&lt;p>It continues to be possible to build different abstractions on top of Kube, but existing workloads are what really benefit users. They have chosen the Kube abstractions deliberately because they are general purpose - rather than describe a completely new system we believe it is more effective to uplevel these existing apps. That means that existing primitives like Service, Deployment, PersistentVolumeClaim, StatefulSet must all require no changes to move from single-cluster to multi-cluster&lt;/p>
&lt;p>By choosing this constraint, we also accept that we will have to be opinionated on making the underlying clusters consistent, and we will have to limit / constrain certain behaviors. Ideally, we focus on preserving the user&amp;rsquo;s view of the changes on a logical cluster, while making the workloads on a physical cluster look more consistent for infrastructure admins. This implies we need to explore both what these workloads might look like (a review of applications) and describe the points of control / abstraction between levels.&lt;/p>
&lt;h3 id="constraint-90-of-application-infrastructure-controllers-should-be-useful-against-kcp">Constraint: 90% of application infrastructure controllers should be useful against &lt;code>kcp&lt;/code>&lt;/h3>
&lt;p>A controller that performs app infra related functions should be useful without change against &lt;code>kcp&lt;/code>. For instance, the etcd operator takes an &lt;code>Etcd&lt;/code> cluster CRD and creates pods. It should be possible for that controller to target a &lt;code>kcp&lt;/code> logical cluster with the CRD and create pods on the logical cluster that are transparently placed onto a cluster.&lt;/p>
&lt;h2 id="areas-of-investigation">Areas of investigation&lt;/h2>
&lt;ol>
&lt;li>Define high level user use cases&lt;/li>
&lt;li>Study approaches from the ecosystem that do not require workloads to change significantly to spread&lt;/li>
&lt;li>Explore characteristics of most common Kube workload objects that could allow them to be transparently placed&lt;/li>
&lt;li>Identify the control points and data flow between workload and physical cluster that would be generally useful across a wide range of approaches - such as:
&lt;ol>
&lt;li>How placement is assigned, altered, and removed (&amp;ldquo;scheduling&amp;rdquo; or &amp;ldquo;placement&amp;rdquo;)&lt;/li>
&lt;li>How workloads are transformed from high level to low level and then summarized back&lt;/li>
&lt;li>Categorize approaches in the ecosystem and gaps where collaboration could improve velocity&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Identify key infrastructure characteristics for multi-cluster
&lt;ol>
&lt;li>Networking between components and transparency of location to movement&lt;/li>
&lt;li>Data movement, placement, and replication&lt;/li>
&lt;li>Abstraction/interception of off-cluster dependencies (external to the system)&lt;/li>
&lt;li>Consistency of infrastructure (where does Kube not sufficiently drive operational consistency)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Seek consensus in user communities on whether the abstractions are practical&lt;/li>
&lt;li>Invest in key technologies in the appropriate projects&lt;/li>
&lt;li>Formalize parts of the prototype into project(s) drawing on the elements above if successful!&lt;/li>
&lt;/ol>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>Representing feedback from a number of multi-cluster users with a diverse set of technologies in play:&lt;/p>
&lt;h3 id="as-a-user">As a user&lt;/h3>
&lt;ol>
&lt;li>I can &lt;code>kubectl apply&lt;/code> a workload that is agnostic to node placement to &lt;code>kcp&lt;/code> and see the workload assigned to real resources and start running and the status summarized back to me.&lt;/li>
&lt;li>I can move an application (defined in 1) between two physical clusters by changing a single high level attribute&lt;/li>
&lt;li>As a user when I move an application (as defined in 2) no disruption of internal or external traffic is visible to my consumers&lt;/li>
&lt;li>As a user I can debug my application in a familiar manner regardless of cluster&lt;/li>
&lt;li>As a user with a stateful application by persistent volumes can move / replicate / be shared across clusters in a manner consistent with my storage type (read-write-one / read-write-many).&lt;/li>
&lt;/ol>
&lt;h3 id="as-an-infrastructure-admin">As an infrastructure admin&lt;/h3>
&lt;ol>
&lt;li>I can decommision an physical cluster and see workloads moved without disruption&lt;/li>
&lt;li>I can set capacity bounds that control admission to a particular cluster and react to workload growth organically&lt;/li>
&lt;/ol>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;p>In the early prototype stage &lt;code>kcp&lt;/code> uses the &lt;code>syncer&lt;/code> and the &lt;code>deployment-splitter&lt;/code> as stand-ins for more complex scheduling and transformation. This section should see more updates in the near term as we move beyond areas 1-2 (use cases and ecosystem research)&lt;/p>
&lt;h3 id="possible-design-simplifications">Possible design simplifications&lt;/h3>
&lt;ol>
&lt;li>Focus on every object having an annotation saying which clusters it is targeted at
&lt;ul>
&lt;li>We can control the annotation via admission eventually, works for all objects&lt;/li>
&lt;li>Tracking declarative and atomic state change (NONE -&amp;gt; A, A-&amp;gt;(A,B), A-&amp;gt;NONE) on
objects&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>RBAC stays at the higher level and applies to the logical clusters, is not synced
&lt;ul>
&lt;li>Implication is that controllers won&amp;rsquo;t be syncable today, BUT that&amp;rsquo;s ok because it&amp;rsquo;s likely giving workloads control over the underlying cluster is a non-goal to start and would have to be explicit opt-in by admin&lt;/li>
&lt;li>Controllers already need to separate input from output - most controllers assume they&amp;rsquo;re the same (but things like service load balancer)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Think of scheduling as a policy at global, per logical cluster, and optionally the namespace (policy of object type -&amp;gt; 0..N clusters)
&lt;ul>
&lt;li>Simplification over doing a bunch of per object work, since we want to be transparent (per object is a future optimization with some limits)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol></description></item></channel></rss>